---
title: çˆ¬å–è¿›å‡»çš„å·¨äººæ¼«ç”»
date: 2020-02-23 15:13:57
lang: zh-Hans
tags: 
- Python
categories: Coding

---

# ä½¿ç”¨Scrapyçˆ¬å–è¿›å‡»çš„å·¨äººæ¼«ç”»

## ä¸€ã€ç®€ä»‹

â€‹		è‡ªå·±çœ‹åˆ°ç½‘ä¸Šæœ‰ä¸¤ä¸ªå¤§ç‰›åˆ†åˆ«çˆ¬å–äº†**åˆæ³•**(Naruto)ä¸**éæ³•**(~~ä½ æ‡‚çš„~~)çš„æ¼«ç”»ï¼Œååˆ†æ„Ÿå¹ï¼Œä¾¿ä¹Ÿæƒ³å€Ÿé‰´å€Ÿé‰´ï¼Œç»“æœå¤§ç‰›çš„çš„ä»£ç åœ¨åšä¸»çš„ç”µè„‘ä¸Šè¿è¡Œä¸äº†(~~ä¸§å°½å¤©è‰¯~~),æ‰€ä»¥å°±åªæœ‰è‡ªå·±å†™äº†ä¸€ä¸ªç®—æ˜¯ç»“åˆç‰ˆçš„ä»£ç ï¼Œçˆ¬å–äº†è¿™ä¸ª[ç½‘ç«™](https://www.fzdm.com/)ã€‚åœ¨æ­¤åˆ†äº«ç»™å¤§å®¶ï¼Œæˆäººä»¥bothğŸŸã€‚

â€‹		ä»£ç å·²ç»æŒ‚åœ¨GitHubä¸Šé¢äº†ï¼Œæƒ³ä¸‹æ¼«ç”»çš„å¯ä»¥æ»‘åˆ°æœ€ä¸‹é¢è§‚çœ‹ä¸‹è½½æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•ä¸ä»…å¯ä»¥ä¸‹è½½è¿›å‡»çš„å·¨äººï¼Œæ•´ä¸ªç½‘ç«™çš„æ¼«ç”»éƒ½å¯ä»¥çˆ¬ï¼Œå»ºè®®å¤§å®¶åˆ«ä¹±æ”¹æˆ‘è®¾ç½®çš„å»¶è¿Ÿï¼Œçˆ¬çš„å¤ªå¿«äº†å¯èƒ½ä¼šè¢«ç½‘ç«™é”IPã€‚

<!--more-->

## äºŒã€ç¯å¢ƒå‡†å¤‡

åšä¸»çš„ç¯å¢ƒå¦‚ä¸‹ï¼š

```
Mavericks-MacBook-Pro:~ maverick$
Python 2.7.10 (default, Feb 22 2019, 21:55:15) 
Scrapy 1.8.0 - no active project
```

åœ¨è¿™é‡Œæˆ‘é»˜è®¤å¤§å®¶éƒ½å·²ç»å®‰è£…å¥½äº†scrapyï¼Œ[ä¼ é€é—¨](https://www.osgeo.cn/scrapy/intro/install.html#intro-install)

ä¸çŸ¥é“å¤§å®¶ä¼šé‡åˆ°ä»€ä¹ˆéº»çƒ¦ï¼Œåšä¸»åªç”¨äº†è¿™ä¸€å¥ä»£ç ï¼š

```
pip install Scrapy
```

## ä¸‰ã€åŸºç¡€å‡†å¤‡

### Scrapyç®€ä»‹ï¼ˆ[å¤§ç‰›çš„æ–‡ç« ](https://blog.csdn.net/c406495762/article/details/72858983)ï¼‰

 	 Scrapy Engine(Scrapyæ ¸å¿ƒ) è´Ÿè´£æ•°æ®æµåœ¨å„ä¸ªç»„ä»¶ä¹‹é—´çš„æµã€‚Spiders(çˆ¬è™«)å‘å‡ºRequestsè¯·æ±‚ï¼Œç»ç”±Scrapy Engine(Scrapyæ ¸å¿ƒ) äº¤ç»™Scheduler(è°ƒåº¦å™¨)ï¼ŒDownloader(ä¸‹è½½å™¨)Scheduler(è°ƒåº¦å™¨) è·å¾—Requestsè¯·æ±‚ï¼Œç„¶åæ ¹æ®Requestsè¯·æ±‚ï¼Œä»ç½‘ç»œä¸‹è½½æ•°æ®ã€‚Downloader(ä¸‹è½½å™¨)çš„Responseså“åº”å†ä¼ é€’ç»™Spidersè¿›è¡Œåˆ†æã€‚æ ¹æ®éœ€æ±‚æå–å‡ºItemsï¼Œäº¤ç»™Item Pipelineè¿›è¡Œä¸‹è½½ã€‚Spiderså’ŒItem Pipelineæ˜¯éœ€è¦ç”¨æˆ·æ ¹æ®å“åº”çš„éœ€æ±‚è¿›è¡Œç¼–å†™çš„ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸¤ä¸ªä¸­é—´ä»¶ï¼ŒDownloaders Mddlewareså’ŒSpider Middlewaresï¼Œè¿™ä¸¤ä¸ªä¸­é—´ä»¶ä¸ºç”¨æˆ·æä¾›æ–¹é¢ï¼Œé€šè¿‡æ’å…¥è‡ªå®šä¹‰ä»£ç æ‰©å±•Scrapyçš„åŠŸèƒ½ï¼Œä¾‹å¦‚å»é‡ç­‰ã€‚

![Scrapy](https://s1.ax1x.com/2020/04/02/GGY3Ct.png)

### åŸºæœ¬æ€è·¯

æ³¨æ„ï¼è¿™ç¯‡æ–‡ç« å¹¶ä¸æ˜¯officialæ–‡ç« ï¼Œä¸€åˆ‡è¿˜ä»¥[å®˜æ–¹æ•™ç¨‹](https://www.osgeo.cn/scrapy/intro/tutorial.html)ä¸ºå‡†ã€‚è¿™é‡Œåªè®²æœ¬æ¬¡æ“ä½œç”¨åˆ°çš„çŸ¥è¯†ã€‚

- åˆ›å»ºä¸€ä¸ªScrapyé¡¹ç›®ï¼›
- å®šä¹‰æå–çš„Itemï¼›
- ç¼–å†™çˆ¬å–ç½‘ç«™çš„ spider å¹¶æå– Itemï¼›
- åˆ©ç”¨pythonè‡ªå¸¦çš„requeståº“è±ä¸‹è½½æ¼«ç”»

## å››ã€ç¬¬äºŒæ¬¡å‡†å¤‡

### åˆ›å»ºé¡¹ç›®

```
scrapy startproject Titan
```

ç„¶åæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿé¡¹ç›®å†…æ¶‰åŠçš„æ–‡ä»¶

```
|____Titan
| |____.DS_Store
| |____scrapy.cfg
| |____Titan
| | |____.DS_Store
| | |____spiders
| | | |____titan_spider.py
| | | |______init__.py
| | | |______pycache__
| | | | |______init__.cpython-38.pyc
| | | | |____titan_spider.cpython-38.pyc
| | | | |____titan_spider.cpython-37.pyc
| | | | |______init__.cpython-37.pyc
| | |______init__.py
| | |______pycache__
| | | |______init__.cpython-38.pyc
| | | |____settings.cpython-38.pyc
| | | |____settings.cpython-37.pyc
| | | |______init__.cpython-37.pyc
| | |____middlewares.py
| | |____settings.py
| | |____items.py
| | |____pipelines.py
```

å¤§éƒ¨åˆ†éƒ½æ²¡å•¥ç”¨ï¼Œé‡ç‚¹æ˜¯æˆ‘ä»¬è¦åœ¨spideré‡Œé¢æ·»åŠ ä¸€ä¸ªè‡ªå·±ç¼–å†™çš„pythonæ–‡ä»¶ï¼Œå¯ä»¥æ˜¯ä»»æ„åå­—ï¼Œåƒæˆ‘å°±å«ä»–å·¨äººèœ˜è››

```
titan_spider.py
```

### åˆ›å»ºspiderç±»

åˆ›å»ºä¸€ä¸ªç”¨æ¥å®ç°å…·ä½“çˆ¬å–åŠŸèƒ½çš„ç±»ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„å¤„ç†å®ç°éƒ½ä¼šåœ¨è¿™ä¸ªç±»ä¸­è¿›è¡Œï¼Œå®ƒå¿…é¡»ä¸º `scrapy.Spider` çš„å­ç±»ã€‚

åœ¨ `Titan/spiders` æ–‡ä»¶è·¯å¾„ä¸‹åˆ›å»º `titan_spider.py` æ–‡ä»¶ã€‚åœ¨é‡Œé¢å°±å¼€å§‹æˆ‘ä»¬èœ˜è››ï¼ˆ~~åªçŒª~~ï¼‰çš„åˆå§‹åŒ–

```python
# -*- coding: utf-8 -*-
#ä¸Šé¢codeæ˜¯ä¸ºäº†è®©å…¶æ”¯æŒä¸­æ–‡
import scrapy#scrapyæœ¬å°Š
import re#ä¿å­˜æ–‡ä»¶çš„library
import time#è®¾ç½®å»¶æ—¶
import requests#ä»ç½‘ç»œä¸‹è½½å›¾ç‰‡
from urllib.request import urlretrieve

class TitanSpider(scrapy.Spider):
	name = "titan"#å®šä¹‰spiderçš„åå­—
	start_urls = ['https://manhua.fzdm.com/132/']#èµ·å§‹é¡µé¢
	allowed_domains = ['https://manhua.fzdm.com','http://p2.manhuapan.com/']#å…è®¸èŒƒå›´
  #ä¸Šé¢çš„åå­—éƒ½æ˜¯officialçš„åå­—åƒä¸‡åˆ«æ”¹
```

### shellåˆ†æ

åœ¨command lineé‡Œé¢è¾“å…¥

```
scrapy shell 'https://manhua.fzdm.com/39'
```

ç„¶åä½ ä¼šå¾—åˆ°è¿™ä¸€å †ä¸œè¥¿ï¼ˆåˆ«ğŸ¦å®ƒï¼‰

```
2020-02-23 20:14:47 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)
2020-02-23 20:14:47 [scrapy.utils.log] INFO: Versions: lxml 4.4.2.0, libxml2 2.9.4, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.8.1 (v3.8.1:1b293b6006, Dec 18 2019, 14:08:53) - [Clang 6.0 (clang-600.0.57)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform macOS-10.14.6-x86_64-i386-64bit
2020-02-23 20:14:47 [scrapy.crawler] INFO: Overridden settings: {'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0}
2020-02-23 20:14:47 [scrapy.extensions.telnet] INFO: Telnet Password: e3528447494d6c3d
2020-02-23 20:14:47 [scrapy.middleware] INFO: Enabled extensions:
...ä¸­é—´çœç•¥...
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x10d0cd760>
[s]   item       {}
[s]   request    <GET https://manhua.fzdm.com/39>
[s]   response   <200 https://manhua.fzdm.com/39//>
[s]   settings   <scrapy.settings.Settings object at 0x10d0cd460>
[s]   spider     <DefaultSpider 'default' at 0x10d573400>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects 
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
>>> 
```

ç„¶åæˆ‘ä»¬å°±è¦ä½¿ç”¨xpathæˆ–è€…æ˜¯csså»å¯»æ‰¾æŒ‡å®šçš„é¡µé¢å†…å®¹ï¼ˆ~~å¥¥åˆ©ç»™å¹²å®ƒ~~ï¼‰

åšä¸»ä¹Ÿå­¦ä¹ äº†ä¸€äº›æ—¶é—´ï¼Œå»ºè®®å„ä½å»åº·åº·è¿™ä¸ª[æ•™ç¨‹](https://www.jianshu.com/p/489c5d21cdc7)(~~æ±‚ä½œè€…ç»™å¹¿å‘Šè´¹æ°é¥­~~)

ç†æ¸…æ€è·¯ï¼Œç°åœ¨æˆ‘ä»¬è¦æ‰¾åˆ°å„è¯çš„urlï¼Œé€šè¿‡è§‚å¯Ÿå‘ç°è¿™äº›urléƒ½åœ¨<a>æ ‡ç­¾ä¸‹

è§‚å¯Ÿæ–¹æ³•ï¼šé¼ æ ‡å³é”®ç„¶åç‚¹å‡»inspectï¼Œå†ç‚¹ä¸€ä¸‹å·¦ä¸Šè§’çš„é€‰æ‹©å™¨å°±å¯ä»¥æŸ¥çœ‹é¡µé¢å…ƒç´ çš„æ‰€åœ¨ä½ç½®äº†

![Inspect](https://s1.ax1x.com/2020/04/02/GGNf91.png)

äºæ˜¯è¾“å…¥

```
response.xpath('//li/a[1]/@href').extract()
```

è·å–åˆ°æ‰€æœ‰ç¬¦åˆè¿™ç§ç‰¹å¾çš„herf

```
>>> response.xpath('//li/a[1]/@href').extract()
['//www.fzdm.com', '//news.fzdm.com', '//manhua.fzdm.com', '126/', '125/', '124/', '123/', '122/', '121/', '120/', '119/', '118/', '117/', '116/', 'qc65/', '115/', 'qc64/', '114/', 'qc63/', '113/', 'qc62/', '112/', 'å‰ä¼ 61/', '111/', 'å‰ä¼ 60/', '110/', 'å‰ä¼ 59/', '109/', '108/', 'å‰ä¼ 57/', '107/', 'å‰ä¼ 56/', '106/', 'å‰ä¼ 55/', '105/', 'å‰ä¼ 54/', '104/', '103/', '102/', 'qz51/', '101/', '100/', 'qz49/', '99/', 'qz48/', '98/', 'qz47/', '97/', 'thf46/', '096/', 'wp45/', '95/', 'qz44/', '94/', 'qz43/', '93/', 'qz42/', '92/', 'qz41/', '91/', 'qz40/', 'qz40/', 'qz39/', 'qz38/', '90/', '89/', '88/', 'qz37/', '87/', ' before-the-fall-36/', '86/', '85/', '84/', '83/', '82/', '81/', '80/', '079/', '078/', '77/', '76/', '75/', 'd74/', '73/', '72/', '71/', '70/', '69/', 'd68/', '67/', '66/', 'dxj52/', '65/', '64/', '63/', '62/', '61/', '60/', '59/', 'wc08/', '58/', 'wc07/', 'qc07/', '57/', 'wc06/', '56/', 'qc06/', '55/', '54/', 'wc04/', '53/', 'wc02/', '52/', 'wc01/', '51/', '50/', 'wc00/', '49/', 'xz/', 'qc01/', '48/', 'fwp/', '47/', 'sgp/', '46/', '45/', '44/', 'fwp02/', 'fwp01/', '043/', '042/', '041/', '040/', '039/', '038/', '037/', '036/', '035/', '034/', '033/', '032/', '031/', '030/', '029/', '028/', '027/', '026/', '025/', '024/', '023/', '022/', '021/', '020/', '019/', '018/', '017/', '016/', '015/', '014/', '013/', '012/', '011/', '010/', '009/', '008/', '007/', '006/', '005/', '004/', '003/', '002/', '001/']
```

æˆ‘ä»¬å‘ç°åˆæœ‰å‡ ä¸ªæµ‘æ°´**æ‘¸é±¼**çš„urlæ··äº†è¿›æ¥ï¼Œä¸è¿‡å’±ä»¬å…ˆæŠŠè¿™ä¸ªæ”¾åœ¨ä¸€è¾¹ï¼Œç­‰ä¼šåœ¨pythoné‡Œé¢ç”¨å­—ç¬¦ä¸²æ“ä½œæŠŠå®ƒä»¬ç»™ç­›æ‰ï¼ˆ~~åšä¸»ä¸ä¼šä¸€æ­¥æ‰¾åˆ°æ­£ç¡®urlçš„æ–¹æ³•qaq~~ï¼‰ï¼Œå¦‚æœæœ‰æ›´å¥½çš„æ–¹æ³•è¯·å¤§ç¥æŒ‡å‡ºï¼ˆå¸¦æˆ‘å¸¦æˆ‘ï¼ï¼‰

 ä½¿ç”¨ctrl+dé€€å‡ºä¹‹å‰çš„shellï¼Œåˆ†æç« èŠ‚é¡µé¢ã€‚è¿™æ¬¡æˆ‘ä»¬éœ€è¦æ‰¾åˆ°å›¾ç‰‡çš„urlä»¥åŠä¸‹ä¸€é¡µçš„url

### å†æ¬¡åˆ†æ

![Inspect](https://raw.githubusercontent.com/MaverickTang/Images/master/%E7%88%AC%E5%8F%96%E8%BF%9B%E5%87%BB%E7%9A%84%E5%B7%A8%E4%BA%BA%E6%BC%AB%E7%94%BB/Inspect2.png)

æ‰‹åŠ¨@é£è½¦åŠ¨æ¼«çš„å¹¿å‘Šå•†åˆ°æˆ‘è¿™é‡Œæ¥æŠŠå¹¿å‘Šè´¹ç»“ä¸€ä¸‹ï¼Œã€æ‰‹åŠ¨ç‹—å¤´ã€‘

è¿™æ¬¡æˆ‘ä»¬æ‰¾ä¸€ä¸‹ä¸‹ä¸€é¡µçš„urlï¼ˆè¿™ä¸ªç½‘ç«™ä»–å›¾ç‰‡çš„urlæ”¾çš„æ¯”è¾ƒæ—¥æ€ªï¼‰

åœ¨command lineé‡Œé¢è¾“å…¥

```
scrapy shell 'https://manhua.fzdm.com/39//126/'
```

ç„¶åæˆ‘ä»¬éœ€è¦å†æ¬¡æ‰¾åˆ° 

```
<a href="index_0.html" class="pure-button button-success">ç¬¬1é¡µ</a>
```

ç„¶åè€å¥—è·¯

```
>>> response.xpath('//a[contains(@href, "index")]/@href').extract()
['index_0.html', 'index_1.html', 'index_2.html', 'index_3.html', 'index_4.html', 'index_5.html', 'index_6.html', 'index_1.html']
```

æˆ‘ä»¬çŸ¥é“æœ€åä¸€ä¸ªurlå°±æ˜¯å’±ä»¬çš„next pageäº†

**ä½†æ˜¯ï¼ï¼ï¼**

æˆ‘ä»¬è¿™ä¹ˆæ‰èƒ½çŸ¥é“è¿™ä¸€ç« ä»€ä¹ˆæ—¶å€™ç»“æŸå‘¢ï¼Ÿ

![Inspect](https://raw.githubusercontent.com/MaverickTang/Images/master/%E7%88%AC%E5%8F%96%E8%BF%9B%E5%87%BB%E7%9A%84%E5%B7%A8%E4%BA%BA%E6%BC%AB%E7%94%BB/Inspect3.png)



è¿™æ˜¯æˆ‘ä»¬çš„æœ€åä¸€é¡µçš„ä»£ç ï¼Œçœ‹èµ·æ¥ä»urlä¸Šä¸€ç‚¹å¤´ç»ªéƒ½æ²¡æœ‰ï¼Œä½†æ˜¯ä»æ—è¾¹çš„æ–‡å­—ä¸Šæˆ‘ä»¬åˆæœ‰äº†æ–°çš„çº¿ç´¢ï¼Œä¸€èˆ¬å®ƒä¼šç»™å‡ºå¦‚ï¼šä¸‹ä¸€é¡µè¿™æ ·çš„ä¿¡æ¯ï¼Œæœ€åä¸€é¡µåˆ™æ²¡æœ‰è¿™æ ·çš„ä¿¡æ¯ï¼Œåªè¦æˆ‘ä»¬çŸ¥é“æ˜¯å¦æœ‰â€œä¸‹ä¸€é¡µâ€ï¼Œæˆ‘ä»¬å°±èƒ½çŸ¥é“æ˜¯å¦ä¸ºæœ€åä¸€é¡µ

![Inspect](https://s1.ax1x.com/2020/04/02/GGN2N9.png)

æ‰€ä»¥è¦è·å–ä¸Šé¢çš„æ–‡å­—ï¼Œä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ï¼š

è¯·çœ‹ç¬¬ä¸€é¡µä¸æœ€åä¸€é¡µçš„å¯¹æ¯”

```
>>> response.xpath('//a[contains(@href, "index")]/text()').extract()
['ç¬¬1é¡µ', '2', '3', '4', '5', '6', '7', 'ä¸‹ä¸€é¡µ']
```

```
>>> response.xpath('//a[contains(@href, "index")]/text()').extract()
['ä¸Šä¸€é¡µ', '40', '41', '42', '43', '44', 'ç¬¬45é¡µ']
```

ç„¶åæ—¢ç„¶æˆ‘ä»¬å·²ç»çŸ¥é“äº†åˆ¤æ–­ä¸‹ä¸€é¡µçš„æ–¹æ³•ï¼Œæ¥ä¸‹æ¥å°±æ˜¯è·å–å›¾ç‰‡é“¾æ¥äº†

### è·å–å›¾ç‰‡é“¾æ¥

![Inspect](https://raw.githubusercontent.com/MaverickTang/Images/master/%E7%88%AC%E5%8F%96%E8%BF%9B%E5%87%BB%E7%9A%84%E5%B7%A8%E4%BA%BA%E6%BC%AB%E7%94%BB/pic.png)

å†æ¬¡é€‰æ‹©æˆ‘ä»¬æ‰¾åˆ°äº†å›¾ç‰‡çš„url

**ä½†æ˜¯**ã€‚ã€‚ã€‚

```
>>> response.xpath('//img/@src').extract()
['https://static.fzdm.com/css/logo.png', 'https://cdn.jsdelivr.net/gh/fzdm/st@75839ec8feb53ac89fe52134fc648a17bd1bd31f/img/loading.gif']
```

wocå±…ç„¶æ‰¾ä¸åˆ°å›¾ç‰‡çš„urlï¼Ÿï¼Ÿï¼Ÿ

![Inspect](https://s1.ax1x.com/2020/04/02/GGNsnU.jpg)

äºæ˜¯åº·åº·è¿™ä¸ªèœ˜è››è·å–åˆ°çš„æ•´ä¸ªhtmlä»£ç 

```
>>> response.body
b'<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta http-equiv="Content-Language" content="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="//www.fzdm.com"><link rel="dns-prefetch" href="//manhua.fzdm.com"><link rel="dns-prefetch" href="//p1.manhuapan.com"><link rel="dns-prefetch" href="//p2.manhuapan.com"><link rel="dns-prefetch" href="//p5.manhuapan.com"><link rel="dns-prefetch" href="//p17.manhuapan.com"><meta content="all" name="robots"><title>\xe8\xbf\x9b\xe5\x87\xbb\xe7\x9a\x84\xe5\xb7\xa8\xe4\xba\xba126\xe8\xaf\x9d 
â€¦â€¦ä»¥ä¸‹çœç•¥
```

æˆ‘ä»¬å¤åˆ¶ä¹‹åæ‰“å¼€ä»»æ„ä»£ç ç¼–è¯‘å™¨ç„¶å`Command+f`å¯»æ‰¾è¿™ä¸ªâ€œ2020/02/08055441539556.jpgâ€urlåœ¨å“ªé‡Œã€‚

![Inspect](https://s1.ax1x.com/2020/04/02/GGN7He.png)

æˆ‘ä»¬å‘ç°è¿™ä¸ªurlæ”¾åœ¨javascripté‡Œé¢ï¼Œä½¿ç”¨`document.write()`ã€‚ã€‚ã€‚

ä½ ä»¥ä¸ºæˆ‘æœ‰ä»€ä¹ˆéªšæ“ä½œï¼Ÿï¼Ÿï¼Ÿ

![Inspect](https://s1.ax1x.com/2020/04/02/GGUpDS.gif)

æˆ‘è¿˜çœŸæ²¡æœ‰ã€‚ã€‚ã€‚

æ‰¾åˆ°script

```
>>> response.xpath('//script/text()').extract()
["if ('serviceWorker' in navigator) {\n      navigator.serviceWorker.register('/sw.js', { scope: '/' }).then(function (registration) {\n        // registration.unregister().then(function(boolean) {\n        // if boolean = true, unregister is successful\n        // });\n        // æ³¨å†ŒæˆåŠŸ\n        /*\n      var serviceWorker;\n      if (registration.installing) {\n        console.log('installing');\n      } else if (registration.waiting) {\n        console.log('waiting');\n      } else if (registration.active) {\n        console.log('active');\n      }\n      */\n        console.log('ServiceWorker registration successful with scope: ', registration.scope);\n      }).catch(function (err) {\n        // æ³¨å†Œå¤±è´¥ :(\n        console.log('ServiceWorker registration failed: ', err);\n        let refreshing = false\n        navigator.serviceWorker.addEventListener('controllerchange', () => {\n          if (refreshing) {\n            return\n
â€¦â€¦ä»¥ä¸‹çœç•¥
```

äºæ˜¯æˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªå¾ˆå¤§çš„array whichæœ‰æˆ‘ä»¬éœ€è¦çš„url

åšä¸»æ˜¯ä¸ªé“æ†¨æ†¨ï¼Œå¼ºè¡Œç”¨pythonçš„æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°äº†è¿™ä¸ªurl

æ­£åˆ™è¡¨è¾¾å¼ä¸ä¼šçš„å¯ä»¥èµ°[è¿™é‡Œ](https://www.runoob.com/python/python-reg-expressions.html)

åœ¨ç¼–ç¨‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬å°±å…ˆè®°å½•ä¸‹è¿™äº›scriptï¼Œç„¶åå†ç»§ç»­æ“ä½œ

```python
pre_img_url = response.xpath('//script/text()').extract()#è®°å½•script
for i in range(len(pre_img_url)):#è®°å½•çš„æ—¶å€™æ˜¯ä»¥arrayå­˜å‚¨çš„
			matchObj = re.search( r'url=\"()\s*(.*)jpg', pre_img_url[i], re.M|re.I)#æ­£åˆ™è¡¨è¾¾å¼å¯»æ‰¾
			if matchObj:
				ppreimgurl = matchObj.group()#é‡Œé¢å°±åŒ…å«äº†æˆ‘ä»¬è¦æ‰¾çš„urlï¼ˆæœ¬ä¾‹æ˜¯â€œ2020/02/08055441539556.jpgâ€ï¼‰
				img_url= 'http://p2.manhuapan.com/' + ppreimgurl[5:len(ppreimgurl)]#åœ¨å‰é¢åŠ ä¸Šå­˜å‚¨å›¾ç‰‡çš„ç½‘å€
```

## äº”ã€å¼€å§‹ç¼–å†™

è¿˜è®°å¾—æˆ‘ä»¬æœ€å¼€å§‹çš„`parse()`å—ï¼Ÿæˆ‘ä»¬ç°åœ¨ç»™ä»–æ·»åŠ ä¸€ç‚¹ä¸œè¥¿

è§£é‡Šéƒ½åœ¨ä»£ç é‡Œé¢

```python
	def parse(self, response):
		link_urls = response.xpath('//li/a[1]/@href').extract()#æ‰¾åˆ°å„è¯çš„url
		names = response.xpath('//li/a[1]/@title').extract()#æ‰¾åˆ°å„è¯çš„åå­—ï¼Œæ–¹ä¾¿å‘½åæ–‡ä»¶å¤¹
    # ä¸‹é¢çš„variableå¯ä»¥ä¸ç®¡
		x=-1
		h=0
		comics_url_list = []
		rnames = []
		base = 'https://manhua.fzdm.com/132/'
		for i in range(len(link_urls)):
			h=bool(re.search(r'\d', link_urls[i]))
			if(h==True):
				x=x+1
				name=names[x]
				url=base + link_urls[i]#å®ƒçš„urlåªæœ‰baseåé¢çš„éƒ¨åˆ†ï¼Œæ‰€ä»¥è¦æŠŠbaseåŠ ä¸Š
				rnames.append(name)#å°†å„è¯çš„åå­—åŠ å…¥ä¸€ä¸ªæ–°çš„array
				comics_url_list.append(url)#å°†urlåŠ å…¥array
#				print("%s :https://www.manhuadui.com %s"%(names[4+x],link_urls[i]))
#				print("%s : %s"%(rnames[x],comics_url_list[x]))	
				
		print('\n>>>>>>>>>>>>>>>>>>> current page comics list <<<<<<<<<<<<<<<<<<<<')
		print(comics_url_list)
		
		for url in comics_url_list:
			yield scrapy.Request(url=url, callback=self.comics_parse, dont_filter=True)#é€šè¿‡ç‰¹æ®Šçš„scrapyä¼ é€’å°†urlä¼ åˆ°ä¸‹ä¸€ä¸ªå‡½æ•°å¯¹ä¸‹ä¸€å±‚ç½‘é¡µè¿›è¡Œçˆ¬å–
      #ä¸€å®šè¦åŠ å…¥dont_filter=Trueï¼Œä¸ç„¶ä¼šå‡ºbugï¼ˆä¸è¿›å…¥ä¸‹ä¸ªå‡½æ•°ï¼‰
			print('>>>>>>>>  parse comics:' + url)
```

æ¥ä¸‹æ¥æˆ‘ä»¬ç¼–å†™`comics_parse(self, response)`å‡½æ•°æ¥å¤„ç†å„è¯çš„url

```python
	def comics_parse(self, response):#å¦ä¸€ä¸ªå‡½æ•°çˆ¬å–ä¸‹å±‚é¡µé¢
		pre_img_url = response.xpath('//script/text()').extract()#è·å–script
		img_url = ''
		ptitle=response.xpath('//title/text()').extract()#è·å–ç« èŠ‚åç§°
		prepage_num=response.xpath('//a[contains(@href, "index")]/text()').extract()#è·å–é¡µé¢åå­—
		page_num=''
		a=0
		for j in range(1,len(prepage_num)):#å¯»æ‰¾page numberæ¥ä½œä¸ºæ–‡ä»¶å
			for _char in prepage_num[j]:#åˆ¤æ–­ä¸­æ–‡å­—ç¬¦æ¥æ‰¾åˆ°å½“å‰é¡µç ï¼ˆå®ƒä¼šæ˜¯â€œç¬¬né¡µâ€ï¼‰
				if '\u4e00' <= _char <= '\u9fa5':
					page_num=prepage_num[j]
					if page_num == 'ä¸‹ä¸€é¡µ':#å¦‚æœæ˜¯â€˜ä¸‹ä¸€é¡µâ€™å«è¡¨ç¤ºå®ƒæ¼è¿‡äº†â€˜ç¬¬ä¸€é¡µâ€™
						page_num='ç¬¬1é¡µ'
					a=1
					break
			if(a==1):
				break	 
		t=ptitle[0]
		index=ptitle[0].find('è¯')#é€šè¿‡æ‰¾åˆ°â€˜è¯â€™æ¥æ‰¾åˆ°ç« èŠ‚çš„åå­—
		title=t[0:(index+1)]#æˆªå–ç« èŠ‚åå­—
#		matchObj = re.search( r'url=\"()\s*(.*)jpg', line, re.M|re.I)
 	 for i in range(len(pre_img_url)):#è®°å½•çš„æ—¶å€™æ˜¯ä»¥arrayå­˜å‚¨çš„      
   		matchObj = re.search( r'url=\"()\s*(.*)jpg', pre_img_url[i], re.M|re.I)#æ­£åˆ™è¡¨è¾¾å¼å¯»æ‰¾      
    	if matchObj:        
      	ppreimgurl = matchObj.group()#é‡Œé¢å°±åŒ…å«äº†æˆ‘ä»¬è¦æ‰¾çš„urlï¼ˆæœ¬ä¾‹æ˜¯â€œ2020/02/08055441539556.jpgâ€ï¼‰        img_url= 'http://p2.manhuapan.com/' + ppreimgurl[5:len(ppreimgurl)]#åœ¨å‰é¢åŠ ä¸Šå­˜å‚¨å›¾ç‰‡çš„ç½‘å€
				self.log('>>>>>>>>>>>å¼€å§‹ä¸‹è½½<<<<<<<<<<<<<')
#				self.save_img(page_num[len(page_num)], title, img_url)
				document = '/Users/maverick/Desktop/test/One punch'
				comics_path = document + '/' + title
				exists = os.path.exists(comics_path)
				if not exists:#å¦‚æœæ²¡æœ‰åˆ›å»ºè¿‡æ–‡ä»¶å¤¹
#					self.log('create document: ' + title)
					os.makedirs(comics_path)
				pic_name = comics_path + '/' + page_num + '.jpg'
				exists = os.path.exists(pic_name)
				if not exists:
					time.sleep(0.1)#å»¶æ—¶é˜²æ­¢é”ip
					urlretrieve(img_url, pic_name)#ä¸‹è½½å›¾ç‰‡
				break		
		pages_urls = response.xpath('//a[contains(@href, "index")]/@href').extract()#æ‰¾åˆ°ä¸‹ä¸€é¡µçš„url
		page_situation = response.xpath('//a[contains(@href, "index")]/text()').extract()#ä¸æ˜¯å¦ä¸ºæœ€åä¸€é¡µæœ‰å…³
		ans=0
		for _char in page_situation[len(page_situation)-1]:#è¿˜æ˜¯é€šè¿‡ä¸­æ–‡æ¥åˆ¤æ–­æ˜¯å¦ä¸ºæœ€åä¸€é¡µ
			if not '\u4e00' <= _char <= '\u9fa5':
				ans=1
		if(ans==0):
			premyfront = response.request.url#æ‰¾åˆ°å½“å‰é¡µé¢çš„urlï¼Œå†é€šè¿‡å­—ç¬¦ä¸²æ“ä½œå¾—åˆ°åŸºç¡€é¡µ
			fenge = premyfront.split('/')
			myfont=''
			for i in range(5):
				myfont=myfont+fenge[i]+'/'
			next_page = myfont+pages_urls[len(pages_urls)-1]#å¾—åˆ°ä¸‹ä¸€é¡µ
			self.log(next_page)
			yield scrapy.Request(next_page, callback=self.comics_parse, dont_filter=True)	#é€’å½’è‡ªå·±
		else:
			self.log('parse comics:' + title + 'finished.')	
```

ç„¶åæˆ‘ä»¬å°±å¯ä»¥æ¬£èµå®ƒçˆ¬å–çš„æ¼«ç”»äº†ã€‚å› ä¸ºæ•´ä¸ªç½‘ç«™çš„æœºåˆ¶æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦ä¿®æ”¹urlåœ°å€ï¼Œå°±å¯ä»¥ä»»æ„çˆ¬å–è‡ªå·±æƒ³çœ‹çš„æ¼«ç”»äº†ã€‚

## äº”ã€åè®°

å¦‚æœæ˜¯è‡ªå·±æƒ³ç”¨çš„è¯ï¼Œä»£ç å·²ç»åœ¨[GitHub](https://github.com/MaverickTang/Attack-on-titan-download)ä¸Šé¢äº†ï¼Œä¸‹è½½ä¸‹æ¥å°±å¯ä»¥ç›´æ¥ç”¨ã€‚

ä¸ä»…æ˜¯å·¨äººï¼Œè¿™ä¸ªçˆ¬è™«è¿˜å¯ä»¥çˆ¬å–æ•´ä¸ªç½‘ç«™ä¸Šçš„å…¶ä»–æ¼«ç”»ï¼Œæ¯”å¦‚ï¼š

ä¸€æ‹³è¶…äººï¼Œç«å½±å¿è€…ï¼Œæµ·è´¼ç‹,é¬¼ç­ä¹‹åˆƒç­‰ã€‚

è¯·æ±‚æ˜Ÿæ˜Ÿâœ¨

ä½¿ç”¨terminalcdåˆ°æ ¹ç›®å½•ç„¶åè¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```
scrapy crawl titan
```

è®°å¾—æŠŠä¿å­˜çš„æœ¬æœºåœ°å€è¿˜æœ‰æƒ³çˆ¬å–çš„æ¼«ç”»åœ°å€æ”¹ä¸€ä¸‹

å½“ç„¶åªè¦ç¼–ç¨‹çš„é€Ÿåº¦å¤Ÿå¿«ï¼Œè¿™ç§ä¸‹è½½é€Ÿåº¦ç»å¯¹æ¯”æŸç½‘ç›˜å¿«å¾—å¤šï¼Œæœ€å…³é”®çš„æ˜¯æ–¹ä¾¿å¹¶ä¸”å¯ä»¥è£…Bã€‚ã€‚ã€‚

![Inspect](https://s1.ax1x.com/2020/04/02/GGNvgP.gif)

æ”¾ä¸Šè‡ªå·±çˆ¬åˆ°çš„å…µé•¿å¸…ç…§å“ˆå“ˆå“ˆå“ˆå“ˆ

![Inspect](https://s1.ax1x.com/2020/04/02/GGNX9I.jpg)

## å…­ã€å‚è€ƒé“¾æ¥åŠç‰ˆæƒè¯´æ˜

åšä¸»æ˜¯ç¬¬ä¸€æ¬¡å†™åšå®¢ï¼Œå¦‚æœä¾µæƒè¯·è”ç³»æˆ‘åˆ é™¤ï¼Œè¿˜æœ‰å¯¹ä¸¤ä¸ªå¤§ä½¬å†™çš„åšå®¢è¡¨ç¤ºè¯šæŒšæ„Ÿè°¢ï¼Œé“¾æ¥ç¬¬ä¸€ä¸ç¬¬äºŒä¸ªä¸ºä¸¤ä¸ªå¤§ä½¬çš„åšå®¢ã€‚

å‚è€ƒé“¾æ¥ï¼š

1(åˆæ³•).https://blog.csdn.net/c406495762/article/details/72858983

2(éæ³•).[https://moshuqi.github.io/2016/09/27/Python%E7%88%AC%E8%99%AB-Scrapy%E6%A1%86%E6%9E%B6/](https://moshuqi.github.io/2016/09/27/Pythonçˆ¬è™«-Scrapyæ¡†æ¶/)

3(æ­£åˆ™è¡¨è¾¾å¼).https://www.runoob.com/python/python-reg-expressions.html

4(xpathä¸csså­¦ä¹ ).https://www.jianshu.com/p/489c5d21cdc7

5(ä¸‹è½½å›¾ç‰‡æ–¹æ³•).https://morvanzhou.github.io/tutorials/data-manipulation/scraping/3-02-download/

6(è¿›å‡»çš„å·¨äººåœ¨çº¿è§‚çœ‹).https://manhua.fzdm.com/39/