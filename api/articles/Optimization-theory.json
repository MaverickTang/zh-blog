{"title":"优化理论","slug":"Optimization-theory","date":"2020-05-10T16:12:50.000Z","updated":"2021-02-08T00:02:19.920Z","comments":true,"path":"api/articles/Optimization-theory.json","excerpt":"简介在特定约束条件下，选择变量值，是目标函数最大化/最小化分类：优化数据离散连续条件无拘束有拘束函数线性非线性目标单目标多目标","covers":["/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121201301798.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121202415914.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/IMG_7F1340CFFEE8-1.jpeg","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121203434245.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121204443599.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121204511639.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121205401244.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190122144234426.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121205801690.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121210243732.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121210422733.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307263727Ezt0.gif.jpeg","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307264725FBQm.gif.png","/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307264821uw1Y.gif.png"],"content":"<html><head></head><body><h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>在特定约束条件下，选择变量值，是目标函数最大化/最小化</p>\n<h2 id=\"分类：\"><a href=\"#分类：\" class=\"headerlink\" title=\"分类：\"></a>分类：</h2><ul>\n<li><p>优化</p>\n<ul>\n<li>数据<ul>\n<li>离散</li>\n<li>连续</li>\n</ul>\n</li>\n<li>条件<ul>\n<li>无拘束</li>\n<li>有拘束</li>\n</ul>\n</li>\n<li>函数<ul>\n<li>线性</li>\n<li>非线性</li>\n</ul>\n</li>\n<li>目标<ul>\n<li>单目标</li>\n<li>多目标</li>\n</ul>\n</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><h2 id=\"梯度下降法（局部最优）\"><a href=\"#梯度下降法（局部最优）\" class=\"headerlink\" title=\"梯度下降法（局部最优）\"></a>梯度下降法（局部最优）</h2><h3 id=\"理论指导\"><a href=\"#理论指导\" class=\"headerlink\" title=\"理论指导\"></a>理论指导</h3><p>基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121201301798.png\" alt=\"20190121201301798\"></p>\n<p>我们同时可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，这个人此时正好拥有测量出最陡峭方向的能力。所以，此人每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。所以需要找到一个合适的测量方向的频率，来确保下山的方向不错误，同时又不至于耗时太多！</p>\n<p>所以首先，我们需要有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释)</p>\n<h4 id=\"前置知识：微分\"><a href=\"#前置知识：微分\" class=\"headerlink\" title=\"前置知识：微分\"></a>前置知识：微分</h4><p>看待微分的意义，可以有不同的角度，最常用的两种是：</p>\n<ul>\n<li>函数图像中，某点的切线的斜率</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>函数的变化率<br>  几个微分的例子：</p>\n<p>学习坡度下降法，必须以微分作为辅助</p>\n<p>博主写过<a href=\"https://mavericreate.top/Zh-Blog/2020/04/28/Calculus/\" target=\"_blank\" rel=\"noopener\">微积分</a>有关的笔记,感兴趣可以去看看</p>\n<p>这里补充一下<strong>多变量微分</strong></p>\n<p>就是对每个变量进行分别微分</p>\n<script type=\"math/tex; mode=display\">\\cfrac{\\partial}{\\partial x}(x^2y^2)=2xy^2</script><script type=\"math/tex; mode=display\">\\cfrac{\\partial}{\\partial y}(-2y^5+z^2)=-10y^4</script><script type=\"math/tex; mode=display\">\\cfrac{\\partial}{\\partial \\theta_2}(5\\theta_1+2\\theta_2-12\\theta_3)=2</script><script type=\"math/tex; mode=display\">\\cfrac{\\partial}{\\partial \\theta_2}(0.55-(5\\theta_1+2\\theta_2-12\\theta_3)=-2</script><h4 id=\"梯度的概念\"><a href=\"#梯度的概念\" class=\"headerlink\" title=\"梯度的概念\"></a>梯度的概念</h4><p>梯度实际上就是多变量微分的一般化。</p>\n</li>\n</ul>\n<p>  <img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121202415914.png\" alt=\"20190121202415914\"></p>\n<p>我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用<>包括起来，说明梯度其实一个向量。</p>\n<p>梯度是微积分中一个很重要的概念，之前提到过梯度的意义</p>\n<ul>\n<li>在<strong>单变量</strong>的函数中，梯度其实就是函数的<strong>微分</strong>，代表着函数在某个给定点的切线的斜率</li>\n<li>在<strong>多变量</strong>函数中，梯度是一个<strong>向量</strong>，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</li>\n</ul>\n<p><strong>这也就说明了为什么我们需要千方百计的求取梯度！</strong>我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点。</p>\n<p><strong>梯度垂直于等值线</strong></p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/IMG_7F1340CFFEE8-1.jpeg\" alt=\"IMG_7F1340CFFEE8-1\"></p>\n<h3 id=\"数学解释\"><a href=\"#数学解释\" class=\"headerlink\" title=\"数学解释\"></a><strong>数学解释</strong></h3><p>J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121203434245.png\" alt=\"20190121203434245\"></p>\n<h4 id=\"α\"><a href=\"#α\" class=\"headerlink\" title=\"α\"></a>α</h4><p>α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！</p>\n<h4 id=\"⚠️\"><a href=\"#⚠️\" class=\"headerlink\" title=\"⚠️\"></a>⚠️</h4><p>梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号；<strong>那么如果时上坡，也就是梯度上升算法，当然就不需要添加负号了。</strong></p>\n<h3 id=\"两个栗子🌰\"><a href=\"#两个栗子🌰\" class=\"headerlink\" title=\"两个栗子🌰\"></a>两个栗子🌰</h3><h4 id=\"单变量函数\"><a href=\"#单变量函数\" class=\"headerlink\" title=\"单变量函数\"></a>单变量函数</h4><p>有一个二次函数</p>\n<script type=\"math/tex; mode=display\">J(x)=x^2</script><p>轻易求出微分</p>\n<script type=\"math/tex; mode=display\">J\\prime(x)=x^2</script><p>然后设置容易起点</p>\n<script type=\"math/tex; mode=display\">x^0=1</script><p>将学习率设置成<script type=\"math/tex\">a</script></p>\n<p>然后根据公式我们进行迭代计算</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121204443599.png\" alt=\"20190121204443599\"></p>\n<p>如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121204511639.png\" alt=\"20190121204511639\"></p>\n<h4 id=\"多变量函数\"><a href=\"#多变量函数\" class=\"headerlink\" title=\"多变量函数\"></a>多变量函数</h4><p>有一个复合函数</p>\n<script type=\"math/tex; mode=display\">J(x)=a^2+b^2</script><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！<br>我们假设初始的起点为：</p>\n<script type=\"math/tex; mode=display\">(1,3)</script><p>假设学习率为：</p>\n<script type=\"math/tex; mode=display\">a=0.1</script><p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/Screen Shot 2020-05-10 at 11.38.38 AM.png\" alt=\"Screen Shot 2020-05-10 at 11.38.38 AM\"></p>\n<p>我们发现，已经基本靠近函数的最小值点</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121205401244.png\" alt=\"20190121205401244\"></p>\n<h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><h4 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>下面我们将用python实现一个简单的梯度下降算法。场景是一个简单的线性回归的例子：假设现在我们有一系列的点，如下图所示：</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190122144234426.png\" alt=\"20190122144234426\"></p>\n<p>我们将用梯度下降法来拟合出这条直线！</p>\n<p>首先，我们需要定义一个代价函数，在此我们选用<a href=\"https://en.wikipedia.org/wiki/Least_squares\" target=\"_blank\" rel=\"noopener\"><strong>均方误差代价函数</strong></a>（也称平方误差代价函数）</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121205801690.png\" alt=\"20190121205801690\"></p>\n<p>此公式中</p>\n<ul>\n<li>m是数据集中数据点的个数，也就是样本数</li>\n<li>½是一个常量，这样是为了在求梯度的时候，二次方乘下来的2就和这里的½抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响</li>\n<li>y 是数据集中每个点的真实y坐标的值，也就是类标签</li>\n<li>h 是我们的预测函数（假设函数），根据每一个输入x，根据Θ 计算得到预测的y值，即<script type=\"math/tex\">h_\\theta(x^i)=\\theta_0+\\theta_1x_{1}^{(i)}</script></li>\n</ul>\n<p>我们可以根据代价函数看到，代价函数中的变量有两个，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量进行微分</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121210243732.png\" alt=\"20190121210243732\"></p>\n<p>明确了代价函数和梯度，以及预测的函数形式。我们就可以开始编写代码了。但在这之前，需要说明一点，就是为了方便代码的编写，我们会将所有的公式都转换为矩阵的形式，python中计算矩阵是非常方便的，同时代码也会变得非常的简洁。<br>为了转换为矩阵的计算，我们观察到预测函数的形式</p>\n<script type=\"math/tex; mode=display\">h_\\theta(x^i)=\\theta_0+\\theta_1x_{1}^{(i)}</script><p>我们有两个变量，为了对这个公式进行矩阵化，我们可以给每一个点x增加一维，这一维的值固定为1，这一维将会乘到Θ0上。这样就方便我们统一矩阵化的计算</p>\n<script type=\"math/tex; mode=display\">(x_{1}^i,x^i)\\to(x_{0}^i,x_{1}^i,y^i)with x_{0}^i=1\\forall i</script><p>然后我们将代价函数和梯度转化为矩阵向量相乘的形式</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121210422733.png\" alt=\"20190121210422733\"></p>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> numpy <span class=\"keyword\">import</span> *</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 首先，我们需要定义数据集和学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据集大小 即20个数据点</span></span><br><span class=\"line\">m = <span class=\"number\">20</span></span><br><span class=\"line\"><span class=\"comment\"># x的坐标以及对应的矩阵</span></span><br><span class=\"line\">X0 = ones((m, <span class=\"number\">1</span>))  <span class=\"comment\"># 生成一个m行1列的向量，也就是x0，全是1</span></span><br><span class=\"line\">X1 = arange(<span class=\"number\">1</span>, m+<span class=\"number\">1</span>).reshape(m, <span class=\"number\">1</span>)  <span class=\"comment\"># 生成一个m行1列的向量，也就是x1，从1到m</span></span><br><span class=\"line\">X = hstack((X0, X1))  <span class=\"comment\"># 按照列堆叠形成数组，其实就是样本数据</span></span><br><span class=\"line\"><span class=\"comment\"># 对应的y坐标</span></span><br><span class=\"line\">Y = array([</span><br><span class=\"line\">    <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">11</span>, <span class=\"number\">8</span>, <span class=\"number\">12</span>,</span><br><span class=\"line\">    <span class=\"number\">11</span>, <span class=\"number\">13</span>, <span class=\"number\">13</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">17</span>, <span class=\"number\">19</span>, <span class=\"number\">21</span></span><br><span class=\"line\">]).reshape(m, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 学习率</span></span><br><span class=\"line\">alpha = <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#接下来我们以矩阵向量的形式定义代价函数和代价函数的梯度</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义代价函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cost_function</span><span class=\"params\">(theta, X, Y)</span>:</span></span><br><span class=\"line\">    diff = dot(X, theta) - Y  <span class=\"comment\"># dot() 数组需要像矩阵那样相乘，就需要用到dot()</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"number\">1</span>/(<span class=\"number\">2</span>*m)) * dot(diff.transpose(), diff)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义代价函数对应的梯度函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient_function</span><span class=\"params\">(theta, X, Y)</span>:</span></span><br><span class=\"line\">    diff = dot(X, theta) - Y</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"number\">1</span>/m) * dot(X.transpose(), diff)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#最后就是算法的核心部分，梯度下降迭代计算</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 梯度下降迭代</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient_descent</span><span class=\"params\">(X, Y, alpha)</span>:</span></span><br><span class=\"line\">    theta = array([<span class=\"number\">1</span>, <span class=\"number\">1</span>]).reshape(<span class=\"number\">2</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">    gradient = gradient_function(theta, X, Y)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">not</span> all(abs(gradient) <= <span class=\"number\">1e-5</span>):</span><br><span class=\"line\">        theta = theta - alpha * gradient</span><br><span class=\"line\">        gradient = gradient_function(theta, X, Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> theta</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#当梯度小于1e-5时，说明已经进入了比较平滑的状态，类似于山谷的状态，这时候再继续迭代效果也不大了，所以这个时候可以退出循环！</span></span><br><span class=\"line\">optimal = gradient_descent(X, Y, alpha)</span><br><span class=\"line\">print(<span class=\"string\">'optimal:'</span>, optimal)</span><br><span class=\"line\">print(<span class=\"string\">'cost function:'</span>, cost_function(optimal, X, Y)[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#通过matplotlib画出图像，</span></span><br><span class=\"line\"><span class=\"comment\"># 根据数据画出对应的图像</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot</span><span class=\"params\">(X, Y, theta)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">    ax = plt.subplot(<span class=\"number\">111</span>)  <span class=\"comment\"># 这是我改的</span></span><br><span class=\"line\">    ax.scatter(X, Y, s=<span class=\"number\">30</span>, c=<span class=\"string\">\"red\"</span>, marker=<span class=\"string\">\"s\"</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">\"X\"</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">\"Y\"</span>)</span><br><span class=\"line\">    x = arange(<span class=\"number\">0</span>, <span class=\"number\">21</span>, <span class=\"number\">0.2</span>)  <span class=\"comment\"># x的范围</span></span><br><span class=\"line\">    y = theta[<span class=\"number\">0</span>] + theta[<span class=\"number\">1</span>]*x</span><br><span class=\"line\">    ax.plot(x, y)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">plot(X1, Y, optimal)</span><br></pre></td></tr></tbody></table></figure>\n<h2 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a>牛顿法</h2><h3 id=\"应用方向\"><a href=\"#应用方向\" class=\"headerlink\" title=\"应用方向\"></a>应用方向</h3><p>1、求方程的根，2、最优化。</p>\n<p><strong>牛顿法的核心思想是对函数进行泰勒展开。</strong></p>\n<h3 id=\"求方程的根\"><a href=\"#求方程的根\" class=\"headerlink\" title=\"求方程的根\"></a>求方程的根</h3><p>并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以迭代求解。</p>\n<p>原理是利用泰勒公式，在<script type=\"math/tex\">x_0</script>处展开，且展开到一阶，即<script type=\"math/tex\">f(x) = f(x_0)+(x－x_0)f'(x_0)</script></p>\n<p>求解方程f(x)=0，即<script type=\"math/tex\">f(x_0)+(x-x_0)*f'(x_0)=0</script>，求解<script type=\"math/tex\">x = x_1=x_0－f(x_0)/f'(x_0)</script>，因为这是利用泰勒公式的一阶展开，<script type=\"math/tex\">f(x) = f(x_0)+(x－x_0)f'(x_0)</script>处并不是完全相等，而是近似相等，这里求得的x1并不能让<script type=\"math/tex\">f(x)=0</script>，只能说f(x1)的值比f(x0)更接近<script type=\"math/tex\">f(x)=0</script>，于是乎，迭代求解的想法就很自然了，可以进而推出<script type=\"math/tex\">x(n+1)=x(n)－f(x(n))/f'(x(n))</script>，通过迭代，这个式子必然在<script type=\"math/tex\">f(x*)=0</script>的时候收敛。整个过程如下图：</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307263727Ezt0.gif.jpeg\" alt=\"0_1307263727Ezt0.gif\"></p>\n<h3 id=\"最优化\"><a href=\"#最优化\" class=\"headerlink\" title=\"最优化\"></a>最优化</h3><p>在最优化的问题中，线性最优化至少可以使用单纯行法求解，但对于非线性优化问题，牛顿法提供了一种求解的办法。假设任务是优化一个目标函数f，求函数f的极大极小问题，可以转化为求解函数f的导数f’=0的问题，这样求可以把优化问题看成方程求解问题（f’=0）。剩下的问题就和第一部分提到的牛顿法求解很相似了。</p>\n<p>这次为了求解f’=0的根，把f(x)的泰勒展开，展开到2阶形式：</p>\n<script type=\"math/tex; mode=display\">f(x+\\Delta x)=f(x)+f'(x)\\Delta x+f''(x)\\Delta x^2/2</script><p>本式成立，且当<script type=\"math/tex\">\\Delta x</script>无限趋近于0时</p>\n<script type=\"math/tex; mode=display\">f'(x)+f''(x)\\Delta x=0</script><p>求解</p>\n<script type=\"math/tex; mode=display\">\\Delta x=-\\cfrac{f'(x_n)}{f''(x_n)}</script><p>迭代公式为：</p>\n<script type=\"math/tex; mode=display\">x_{n+1}=x_n-\\cfrac{f'(x_n)}{f''(x_n)},n=0,1...</script><p>一般认为牛顿法可以利用到曲线本身的信息，比梯度下降法更容易收敛（迭代更少次数），如下图是一个最小化一个目标方程的例子，红色曲线是利用牛顿法迭代求解，绿色曲线是利用梯度下降法求解。</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307264725FBQm.gif.png\" alt=\"0_1307264725FBQm.gif\"></p>\n<p>在上面讨论的是2维情况，高维情况的牛顿迭代公式是：</p>\n<script type=\"math/tex; mode=display\">x_{n+1}=x_n-[Hf(x_n)]^{-1}\\nabla f(x_n),n\\ge0</script><p>其中H是hessian矩阵，定义为</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307264821uw1Y.gif.png\" alt=\"0_1307264821uw1Y.gif\"></p>\n<p>高维情况依然可以用牛顿迭代求解，但是问题是Hessian矩阵引入的复杂性，使得牛顿迭代求解的难度大大增加，但是已经有了解决这个问题的办法就是Quasi-Newton methond，不再直接计算hessian矩阵，而是每一步的时候使用梯度向量更新hessian矩阵的近似。</p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>1.<a href=\"https://blog.csdn.net/qq_41800366/article/details/86583789\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/qq_41800366/article/details/86583789</a></p>\n<p>2.<a href=\"https://blog.csdn.net/michaelhan3/article/details/82350047\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/michaelhan3/article/details/82350047</a></p>\n</body></html>","more":"<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><h2 id=\"梯度下降法（局部最优）\"><a href=\"#梯度下降法（局部最优）\" class=\"headerlink\" title=\"梯度下降法（局部最优）\"></a>梯度下降法（局部最优）</h2><h3 id=\"理论指导\"><a href=\"#理论指导\" class=\"headerlink\" title=\"理论指导\"></a>理论指导</h3><p>基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走，同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121201301798.png\" alt=\"20190121201301798\"></p>\n<p>我们同时可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，这个人此时正好拥有测量出最陡峭方向的能力。所以，此人每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。所以需要找到一个合适的测量方向的频率，来确保下山的方向不错误，同时又不至于耗时太多！</p>\n<p>所以首先，我们需要有一个可微分的函数。这个函数就代表着一座山。我们的目标就是找到这个函数的最小值，也就是山底。根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是找到给定点的梯度 ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向(在后面会详细解释)</p>\n<h4 id=\"前置知识：微分\"><a href=\"#前置知识：微分\" class=\"headerlink\" title=\"前置知识：微分\"></a>前置知识：微分</h4><p>看待微分的意义，可以有不同的角度，最常用的两种是：</p>\n<ul>\n<li>函数图像中，某点的切线的斜率</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>函数的变化率<br>  几个微分的例子：</p>\n<p>学习坡度下降法，必须以微分作为辅助</p>\n<p>博主写过<a href=\"https://mavericreate.top/Zh-Blog/2020/04/28/Calculus/\" target=\"_blank\" rel=\"noopener\">微积分</a>有关的笔记,感兴趣可以去看看</p>\n<p>这里补充一下<strong>多变量微分</strong></p>\n<p>就是对每个变量进行分别微分</p>\n<script type=\"math/tex; mode=display\">\\cfrac{\\partial}{\\partial x}(x^2y^2)=2xy^2</script><script type=\"math/tex; mode=display\">\\cfrac{\\partial}{\\partial y}(-2y^5+z^2)=-10y^4</script><script type=\"math/tex; mode=display\">\\cfrac{\\partial}{\\partial \\theta_2}(5\\theta_1+2\\theta_2-12\\theta_3)=2</script><script type=\"math/tex; mode=display\">\\cfrac{\\partial}{\\partial \\theta_2}(0.55-(5\\theta_1+2\\theta_2-12\\theta_3)=-2</script><h4 id=\"梯度的概念\"><a href=\"#梯度的概念\" class=\"headerlink\" title=\"梯度的概念\"></a>梯度的概念</h4><p>梯度实际上就是多变量微分的一般化。</p>\n</li>\n</ul>\n<p>  <img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121202415914.png\" alt=\"20190121202415914\"></p>\n<p>我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用&lt;&gt;包括起来，说明梯度其实一个向量。</p>\n<p>梯度是微积分中一个很重要的概念，之前提到过梯度的意义</p>\n<ul>\n<li>在<strong>单变量</strong>的函数中，梯度其实就是函数的<strong>微分</strong>，代表着函数在某个给定点的切线的斜率</li>\n<li>在<strong>多变量</strong>函数中，梯度是一个<strong>向量</strong>，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</li>\n</ul>\n<p><strong>这也就说明了为什么我们需要千方百计的求取梯度！</strong>我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点。</p>\n<p><strong>梯度垂直于等值线</strong></p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/IMG_7F1340CFFEE8-1.jpeg\" alt=\"IMG_7F1340CFFEE8-1\"></p>\n<h3 id=\"数学解释\"><a href=\"#数学解释\" class=\"headerlink\" title=\"数学解释\"></a><strong>数学解释</strong></h3><p>J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121203434245.png\" alt=\"20190121203434245\"></p>\n<h4 id=\"α\"><a href=\"#α\" class=\"headerlink\" title=\"α\"></a>α</h4><p>α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！</p>\n<h4 id=\"⚠️\"><a href=\"#⚠️\" class=\"headerlink\" title=\"⚠️\"></a>⚠️</h4><p>梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号；<strong>那么如果时上坡，也就是梯度上升算法，当然就不需要添加负号了。</strong></p>\n<h3 id=\"两个栗子🌰\"><a href=\"#两个栗子🌰\" class=\"headerlink\" title=\"两个栗子🌰\"></a>两个栗子🌰</h3><h4 id=\"单变量函数\"><a href=\"#单变量函数\" class=\"headerlink\" title=\"单变量函数\"></a>单变量函数</h4><p>有一个二次函数</p>\n<script type=\"math/tex; mode=display\">J(x)=x^2</script><p>轻易求出微分</p>\n<script type=\"math/tex; mode=display\">J\\prime(x)=x^2</script><p>然后设置容易起点</p>\n<script type=\"math/tex; mode=display\">x^0=1</script><p>将学习率设置成<script type=\"math/tex\">a</script></p>\n<p>然后根据公式我们进行迭代计算</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121204443599.png\" alt=\"20190121204443599\"></p>\n<p>如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121204511639.png\" alt=\"20190121204511639\"></p>\n<h4 id=\"多变量函数\"><a href=\"#多变量函数\" class=\"headerlink\" title=\"多变量函数\"></a>多变量函数</h4><p>有一个复合函数</p>\n<script type=\"math/tex; mode=display\">J(x)=a^2+b^2</script><p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！<br>我们假设初始的起点为：</p>\n<script type=\"math/tex; mode=display\">(1,3)</script><p>假设学习率为：</p>\n<script type=\"math/tex; mode=display\">a=0.1</script><p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/Screen Shot 2020-05-10 at 11.38.38 AM.png\" alt=\"Screen Shot 2020-05-10 at 11.38.38 AM\"></p>\n<p>我们发现，已经基本靠近函数的最小值点</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121205401244.png\" alt=\"20190121205401244\"></p>\n<h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><h4 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>下面我们将用python实现一个简单的梯度下降算法。场景是一个简单的线性回归的例子：假设现在我们有一系列的点，如下图所示：</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190122144234426.png\" alt=\"20190122144234426\"></p>\n<p>我们将用梯度下降法来拟合出这条直线！</p>\n<p>首先，我们需要定义一个代价函数，在此我们选用<a href=\"https://en.wikipedia.org/wiki/Least_squares\" target=\"_blank\" rel=\"noopener\"><strong>均方误差代价函数</strong></a>（也称平方误差代价函数）</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121205801690.png\" alt=\"20190121205801690\"></p>\n<p>此公式中</p>\n<ul>\n<li>m是数据集中数据点的个数，也就是样本数</li>\n<li>½是一个常量，这样是为了在求梯度的时候，二次方乘下来的2就和这里的½抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响</li>\n<li>y 是数据集中每个点的真实y坐标的值，也就是类标签</li>\n<li>h 是我们的预测函数（假设函数），根据每一个输入x，根据Θ 计算得到预测的y值，即<script type=\"math/tex\">h_\\theta(x^i)=\\theta_0+\\theta_1x_{1}^{(i)}</script></li>\n</ul>\n<p>我们可以根据代价函数看到，代价函数中的变量有两个，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量进行微分</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121210243732.png\" alt=\"20190121210243732\"></p>\n<p>明确了代价函数和梯度，以及预测的函数形式。我们就可以开始编写代码了。但在这之前，需要说明一点，就是为了方便代码的编写，我们会将所有的公式都转换为矩阵的形式，python中计算矩阵是非常方便的，同时代码也会变得非常的简洁。<br>为了转换为矩阵的计算，我们观察到预测函数的形式</p>\n<script type=\"math/tex; mode=display\">h_\\theta(x^i)=\\theta_0+\\theta_1x_{1}^{(i)}</script><p>我们有两个变量，为了对这个公式进行矩阵化，我们可以给每一个点x增加一维，这一维的值固定为1，这一维将会乘到Θ0上。这样就方便我们统一矩阵化的计算</p>\n<script type=\"math/tex; mode=display\">(x_{1}^i,x^i)\\to(x_{0}^i,x_{1}^i,y^i)with x_{0}^i=1\\forall i</script><p>然后我们将代价函数和梯度转化为矩阵向量相乘的形式</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/20190121210422733.png\" alt=\"20190121210422733\"></p>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> numpy <span class=\"keyword\">import</span> *</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 首先，我们需要定义数据集和学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据集大小 即20个数据点</span></span><br><span class=\"line\">m = <span class=\"number\">20</span></span><br><span class=\"line\"><span class=\"comment\"># x的坐标以及对应的矩阵</span></span><br><span class=\"line\">X0 = ones((m, <span class=\"number\">1</span>))  <span class=\"comment\"># 生成一个m行1列的向量，也就是x0，全是1</span></span><br><span class=\"line\">X1 = arange(<span class=\"number\">1</span>, m+<span class=\"number\">1</span>).reshape(m, <span class=\"number\">1</span>)  <span class=\"comment\"># 生成一个m行1列的向量，也就是x1，从1到m</span></span><br><span class=\"line\">X = hstack((X0, X1))  <span class=\"comment\"># 按照列堆叠形成数组，其实就是样本数据</span></span><br><span class=\"line\"><span class=\"comment\"># 对应的y坐标</span></span><br><span class=\"line\">Y = array([</span><br><span class=\"line\">    <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">5</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>, <span class=\"number\">11</span>, <span class=\"number\">8</span>, <span class=\"number\">12</span>,</span><br><span class=\"line\">    <span class=\"number\">11</span>, <span class=\"number\">13</span>, <span class=\"number\">13</span>, <span class=\"number\">16</span>, <span class=\"number\">17</span>, <span class=\"number\">18</span>, <span class=\"number\">17</span>, <span class=\"number\">19</span>, <span class=\"number\">21</span></span><br><span class=\"line\">]).reshape(m, <span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 学习率</span></span><br><span class=\"line\">alpha = <span class=\"number\">0.01</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#接下来我们以矩阵向量的形式定义代价函数和代价函数的梯度</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义代价函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cost_function</span><span class=\"params\">(theta, X, Y)</span>:</span></span><br><span class=\"line\">    diff = dot(X, theta) - Y  <span class=\"comment\"># dot() 数组需要像矩阵那样相乘，就需要用到dot()</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"number\">1</span>/(<span class=\"number\">2</span>*m)) * dot(diff.transpose(), diff)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义代价函数对应的梯度函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient_function</span><span class=\"params\">(theta, X, Y)</span>:</span></span><br><span class=\"line\">    diff = dot(X, theta) - Y</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"number\">1</span>/m) * dot(X.transpose(), diff)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#最后就是算法的核心部分，梯度下降迭代计算</span></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 梯度下降迭代</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gradient_descent</span><span class=\"params\">(X, Y, alpha)</span>:</span></span><br><span class=\"line\">    theta = array([<span class=\"number\">1</span>, <span class=\"number\">1</span>]).reshape(<span class=\"number\">2</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">    gradient = gradient_function(theta, X, Y)</span><br><span class=\"line\">    <span class=\"keyword\">while</span> <span class=\"keyword\">not</span> all(abs(gradient) &lt;= <span class=\"number\">1e-5</span>):</span><br><span class=\"line\">        theta = theta - alpha * gradient</span><br><span class=\"line\">        gradient = gradient_function(theta, X, Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> theta</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#当梯度小于1e-5时，说明已经进入了比较平滑的状态，类似于山谷的状态，这时候再继续迭代效果也不大了，所以这个时候可以退出循环！</span></span><br><span class=\"line\">optimal = gradient_descent(X, Y, alpha)</span><br><span class=\"line\">print(<span class=\"string\">'optimal:'</span>, optimal)</span><br><span class=\"line\">print(<span class=\"string\">'cost function:'</span>, cost_function(optimal, X, Y)[<span class=\"number\">0</span>][<span class=\"number\">0</span>])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#通过matplotlib画出图像，</span></span><br><span class=\"line\"><span class=\"comment\"># 根据数据画出对应的图像</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">plot</span><span class=\"params\">(X, Y, theta)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">    ax = plt.subplot(<span class=\"number\">111</span>)  <span class=\"comment\"># 这是我改的</span></span><br><span class=\"line\">    ax.scatter(X, Y, s=<span class=\"number\">30</span>, c=<span class=\"string\">\"red\"</span>, marker=<span class=\"string\">\"s\"</span>)</span><br><span class=\"line\">    plt.xlabel(<span class=\"string\">\"X\"</span>)</span><br><span class=\"line\">    plt.ylabel(<span class=\"string\">\"Y\"</span>)</span><br><span class=\"line\">    x = arange(<span class=\"number\">0</span>, <span class=\"number\">21</span>, <span class=\"number\">0.2</span>)  <span class=\"comment\"># x的范围</span></span><br><span class=\"line\">    y = theta[<span class=\"number\">0</span>] + theta[<span class=\"number\">1</span>]*x</span><br><span class=\"line\">    ax.plot(x, y)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\"></span><br><span class=\"line\">plot(X1, Y, optimal)</span><br></pre></td></tr></table></figure>\n<h2 id=\"牛顿法\"><a href=\"#牛顿法\" class=\"headerlink\" title=\"牛顿法\"></a>牛顿法</h2><h3 id=\"应用方向\"><a href=\"#应用方向\" class=\"headerlink\" title=\"应用方向\"></a>应用方向</h3><p>1、求方程的根，2、最优化。</p>\n<p><strong>牛顿法的核心思想是对函数进行泰勒展开。</strong></p>\n<h3 id=\"求方程的根\"><a href=\"#求方程的根\" class=\"headerlink\" title=\"求方程的根\"></a>求方程的根</h3><p>并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以迭代求解。</p>\n<p>原理是利用泰勒公式，在<script type=\"math/tex\">x_0</script>处展开，且展开到一阶，即<script type=\"math/tex\">f(x) = f(x_0)+(x－x_0)f'(x_0)</script></p>\n<p>求解方程f(x)=0，即<script type=\"math/tex\">f(x_0)+(x-x_0)*f'(x_0)=0</script>，求解<script type=\"math/tex\">x = x_1=x_0－f(x_0)/f'(x_0)</script>，因为这是利用泰勒公式的一阶展开，<script type=\"math/tex\">f(x) = f(x_0)+(x－x_0)f'(x_0)</script>处并不是完全相等，而是近似相等，这里求得的x1并不能让<script type=\"math/tex\">f(x)=0</script>，只能说f(x1)的值比f(x0)更接近<script type=\"math/tex\">f(x)=0</script>，于是乎，迭代求解的想法就很自然了，可以进而推出<script type=\"math/tex\">x(n+1)=x(n)－f(x(n))/f'(x(n))</script>，通过迭代，这个式子必然在<script type=\"math/tex\">f(x*)=0</script>的时候收敛。整个过程如下图：</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307263727Ezt0.gif.jpeg\" alt=\"0_1307263727Ezt0.gif\"></p>\n<h3 id=\"最优化\"><a href=\"#最优化\" class=\"headerlink\" title=\"最优化\"></a>最优化</h3><p>在最优化的问题中，线性最优化至少可以使用单纯行法求解，但对于非线性优化问题，牛顿法提供了一种求解的办法。假设任务是优化一个目标函数f，求函数f的极大极小问题，可以转化为求解函数f的导数f’=0的问题，这样求可以把优化问题看成方程求解问题（f’=0）。剩下的问题就和第一部分提到的牛顿法求解很相似了。</p>\n<p>这次为了求解f’=0的根，把f(x)的泰勒展开，展开到2阶形式：</p>\n<script type=\"math/tex; mode=display\">f(x+\\Delta x)=f(x)+f'(x)\\Delta x+f''(x)\\Delta x^2/2</script><p>本式成立，且当<script type=\"math/tex\">\\Delta x</script>无限趋近于0时</p>\n<script type=\"math/tex; mode=display\">f'(x)+f''(x)\\Delta x=0</script><p>求解</p>\n<script type=\"math/tex; mode=display\">\\Delta x=-\\cfrac{f'(x_n)}{f''(x_n)}</script><p>迭代公式为：</p>\n<script type=\"math/tex; mode=display\">x_{n+1}=x_n-\\cfrac{f'(x_n)}{f''(x_n)},n=0,1...</script><p>一般认为牛顿法可以利用到曲线本身的信息，比梯度下降法更容易收敛（迭代更少次数），如下图是一个最小化一个目标方程的例子，红色曲线是利用牛顿法迭代求解，绿色曲线是利用梯度下降法求解。</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307264725FBQm.gif.png\" alt=\"0_1307264725FBQm.gif\"></p>\n<p>在上面讨论的是2维情况，高维情况的牛顿迭代公式是：</p>\n<script type=\"math/tex; mode=display\">x_{n+1}=x_n-[Hf(x_n)]^{-1}\\nabla f(x_n),n\\ge0</script><p>其中H是hessian矩阵，定义为</p>\n<p><img data-src=\"/Users/maverick/Desktop/PROGRAM/Mavericreate/Zh-Blog/source/_posts/Optimization-theory/0_1307264821uw1Y.gif.png\" alt=\"0_1307264821uw1Y.gif\"></p>\n<p>高维情况依然可以用牛顿迭代求解，但是问题是Hessian矩阵引入的复杂性，使得牛顿迭代求解的难度大大增加，但是已经有了解决这个问题的办法就是Quasi-Newton methond，不再直接计算hessian矩阵，而是每一步的时候使用梯度向量更新hessian矩阵的近似。</p>\n<h1 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h1><p>1.<a href=\"https://blog.csdn.net/qq_41800366/article/details/86583789\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/qq_41800366/article/details/86583789</a></p>\n<p>2.<a href=\"https://blog.csdn.net/michaelhan3/article/details/82350047\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/michaelhan3/article/details/82350047</a></p>","categories":[{"name":"技术","path":"api/categories/技术.json"}],"tags":[{"name":"数学","path":"api/tags/数学.json"}]}